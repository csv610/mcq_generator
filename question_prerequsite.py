import litellm
from prompt_builder import PromptBuilder

class QuestionPrerequisite:
    def __init__(self, model: str):
        """
        Initializes the QuestionPrerequisite with a specified model.

        Args:
            model: Model name string (e.g., "openai/gpt-4o-mini").
        """
        self.model = model
        self.prompt_builder = PromptBuilder()

    def question_prerequisites(self, question: str, options: list[str]) -> list[str]:
        """
        Generates prerequisites for a given question based on provided options.

        Args:
            question (str): The question for which prerequisites are to be generated.
            options (list[str]): A list of answer options related to the question.

        Returns:
            list[str]: A list of prerequisite strings generated by the model.

        Raises:
            RuntimeError: If prompt generation or model response retrieval fails.
            ValueError: If the response is not a list of strings.
        """
        try:
            prompt = self.prompt_builder.get_prerequisites_prompt(question, options)
        except Exception as e:
            raise RuntimeError(f"Failed to generate prompt: {e}")

        try:
            response = litellm.completion(
                model=self.model,
                messages=[{"role": "user", "content": prompt}]
            )
            content = response.choices[0].message.content
            message_contents = content.strip().split("\n\n")
        except Exception as e:
            raise RuntimeError(f"Failed to get response from model: {e}")

        return message_contents  # Return the message contents

if __name__ == "__main__":
    model = "openai/gpt-4o-mini"
    question_prerequisite = QuestionPrerequisite(model)

    question = "What is the unit of force?"
    options = ["A) Joule", "B) Newton", "C) Pascal", "D) Watt"]
    response = question_prerequisite.question_prerequisites(question, options)
    print(response)
